# -*- coding: utf-8 -*-
"""WaveletCNN.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1ZQB_QStPeva6zx9r-smZn3W8mYMSsjPZ
"""

import matplotlib.pyplot as plt
from keras.layers import (
    Input, Dense, Conv2D, Flatten, Reshape, Dropout, 
    Activation, AveragePooling2D, BatchNormalization,
    Lambda, Concatenate
)
from keras.models import Model, load_model
from keras.utils.vis_utils import plot_model
from keras.callbacks import ModelCheckpoint
from tensorflow.keras.utils import image_dataset_from_directory
import numpy as np
from gc import collect

def WaveletTransform(img):
    low = img[:,::2,...] + img[:,1::2,...]
    low = (low[:,:,::2,...] + low[:,:,1::2,...]) / 4
    diag = img[:,::2,...] - img[:,1::2,...]
    diag = diag[:,:,::2,...] - diag[:,:,1::2,...]
    top = img[:,::2,...] - img[:,1::2,...]
    top = (top[:,:,::2] + top[:,:,1::2] ) / 2
    left = img[:,:,::2,...] - img[:,:,1::2,...]
    left = (left[:,::2] + left[:,1::2] ) / 2
    
    return low, diag, top, left


def conv_layer(_in, N):
    conv1 = Conv2D(N, kernel_size=(3,3), padding='same')(_in)
    norm1 = BatchNormalization()(conv1)
    relu1 = Activation('relu')(norm1)
    conv2 = Conv2D(N, kernel_size=(3,3), strides=(2,2), padding='same')(relu1)
    norm2 = BatchNormalization()(conv2)
    relu2 = Activation('relu')(norm2)
    return relu2


def build_model(input_shape = (256, 256, 3), num_classes=50):
    _input = BatchNormalization()(Input(input_shape))
    
    low1, diag1, top1, left1 = Lambda(WaveletTransform, name='wavelet_1')(_input)
    low2, diag2, top2, left2 = Lambda(WaveletTransform, name='wavelet_2')(low1)
    low3, diag3, top3, left3 = Lambda(WaveletTransform, name='wavelet_3')(low2)
    low4, diag4, top4, left4 = Lambda(WaveletTransform, name='wavelet_4')(low3)
    
    k1 = Concatenate()([conv_layer(_input,64), low1, diag1, top1, left1])
    k2 = Concatenate()([conv_layer(k1, 128), low2, diag2, top2, left2])
    k3 = Concatenate()([conv_layer(k2, 256), low3, diag3, top3, left3])
    k4 = Concatenate()([conv_layer(k3, 512), low4, diag4, top4, left4])
    
    avg_pool = AveragePooling2D(pool_size=(7,7), strides=1, padding='same')(k4)
    flat = Flatten()(avg_pool)
    output = Dense(num_classes, activation='softmax',name='fc')(flat)
    model = Model(inputs=_input, outputs=output)
    return model

import tensorflow as tf
from tensorflow.python.client import device_lib
print(device_lib.list_local_devices())
print(tf.__version__)
print("Num GPUs Available: ", len(tf.config.experimental.list_physical_devices('GPU')))
print(tf.test.is_gpu_available())

DO_KTH = False
DO_DTD = True

if DO_KTH:
  full_dataset = image_dataset_from_directory('./KTH-TIPS2-b', label_mode='categorical')
  print(full_dataset.class_names)
  DATASET_SIZE = 4752
  train_size = int(0.7 * DATASET_SIZE / 32)
  val_size = int(0.15 * DATASET_SIZE / 32)
  test_size = int(0.15 * DATASET_SIZE / 32)

  full_dataset = full_dataset.shuffle(10_000)
  train_dataset = full_dataset.take(train_size)
  test_dataset = full_dataset.skip(train_size)
  val_dataset = test_dataset.skip(test_size)
  test_dataset = test_dataset.take(test_size)
  # del full_dataset
  n = next(train_dataset.as_numpy_iterator())
  shape, num_classes = n[0].shape[1:], n[1].shape[-1]
  model = build_model(input_shape=shape, num_classes=num_classes)

  print(model.summary())

  model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])

  !rm -rf ./checkpoints/*

  collect()

  checkpoint = ModelCheckpoint('./checkpoints/', monitor='accuracy', save_best_only=True)
  model.fit(train_dataset, validation_data=val_dataset, epochs=5, callbacks=[checkpoint])
  model = load_model('./checkpoints/')
  score = model.evaluate(test_dataset)
  N = sum([_[1].shape[1] for _ in test_dataset.as_numpy_iterator()])
  print(score)
  print(1/np.sqrt(N))



# !cp -r ./drive/MyDrive/dtd .

if DO_DTD:
  full_dataset = image_dataset_from_directory('./dtd/images/', label_mode='categorical')
  print(full_dataset.class_names)
  DATASET_SIZE = 5640
  train_size = int(0.7 * DATASET_SIZE / 32)
  val_size = int(0.15 * DATASET_SIZE / 32)
  test_size = int(0.15 * DATASET_SIZE / 32)

  full_dataset = full_dataset.shuffle(10_000)
  train_dataset = full_dataset.take(train_size)
  test_dataset = full_dataset.skip(train_size)
  val_dataset = test_dataset.skip(test_size)
  test_dataset = test_dataset.take(test_size)
  # del full_dataset
  shape, num_classes = full_dataset.element_spec
  shape, num_classes = tuple(shape.shape[1:]), int(num_classes.shape[1])
  model = build_model(input_shape=shape, num_classes=num_classes)

  print(model.summary())

  model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])

  collect()

  checkpoint = ModelCheckpoint('./checkpoints/', monitor='accuracy', save_best_only=True)
  model.fit(train_dataset, validation_data=val_dataset, epochs=100, callbacks=[checkpoint])
  model = load_model('./checkpoints/')
  score = model.evaluate(test_dataset)
  N = sum([_[1].shape[1] for _ in test_dataset.as_numpy_iterator()])
  print(score)
  print(1/np.sqrt(N))

